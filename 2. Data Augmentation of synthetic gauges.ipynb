{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Techniques\n",
    "* Scaling\n",
    "* Translation\n",
    "* Rotation (at 90 degrees)\n",
    "* Rotation (at finer angles)\n",
    "* Flipping\n",
    "* Adding Salt and Pepper noise\n",
    "* Lighting condition\n",
    "* Perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from math import floor, ceil, pi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect image file paths\n",
    "def get_image_paths ():\n",
    "    folder = './synthetic'\n",
    "    files = os.listdir(folder)\n",
    "    files.sort()\n",
    "    files = ['{}/{}'.format(folder, file) for file in files]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_paths = get_image_paths()\n",
    "print(X_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Image Resizing\n",
    "def tf_resize_images(X_img_file_paths):\n",
    "    X_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (None, None, 3))\n",
    "    tf_img = tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), \n",
    "                                    tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Each image is resized individually as different image may be of different size.\n",
    "        for index, file_path in enumerate(X_img_file_paths):\n",
    "            img = mpimg.imread(file_path)[:, :, :3] # Do not read alpha channel.\n",
    "            resized_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "            X_data.append(resized_img)\n",
    "\n",
    "    X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imgs = tf_resize_images(X_img_paths)\n",
    "print(X_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mpimg.imread(X_img_paths[0])[:,:,:3])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_imgs[0])\n",
    "plt.title('Resized Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_images(X_imgs, n_cols = 4):\n",
    "    n_rows = ceil(len(X_imgs) / n_cols)\n",
    "    display_image = np.zeros([n_rows * IMAGE_SIZE, n_cols * IMAGE_SIZE, 3], dtype = np.float32)\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            X_img = X_imgs[i * n_cols + j]\n",
    "            disp_padded = np.pad(X_img, ((i * IMAGE_SIZE, (n_rows - 1 - i) * IMAGE_SIZE),\n",
    "                                         (j * IMAGE_SIZE, (n_cols - 1 - j) * IMAGE_SIZE), (0, 0)), 'constant')\n",
    "            display_image = np.add(display_image, disp_padded)\n",
    "\n",
    "    plt.figure(figsize = (n_rows * 3, n_cols * 3))\n",
    "    plt.imshow(display_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_all_images(X_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling:\n",
    "Having differently scaled object of interest in the images is the most important aspect of image diversity. When your network is in hands of real users, the object in the image can be tiny or large. Also, sometimes, object can cover the entire image and yet will not be present totally in image (i.e cropped at edges of object). The code shows scaling of image centrally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_scale_images(X_imgs, scales):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
    "    crop_size = np.array([IMAGE_SIZE, IMAGE_SIZE], dtype = np.int32)\n",
    "    \n",
    "    X_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for img_data in X_imgs:\n",
    "            batch_img = np.expand_dims(img_data, axis = 0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "    \n",
    "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
    "    return X_scale_data\n",
    "\t\n",
    "# Produce each image at scaling of 90%, 75% and 60% of original image.\n",
    "# scaled_imgs = central_scale_images(X_imgs, [0.90, 0.75, 0.60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce each image at scaling of 90%, 75% and 60% of original image.\n",
    "scaled_imgs = central_scale_images(X_imgs, [0.90, 0.75, 0.60])\n",
    "print(scaled_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  ==  plt.subplots(figsize = (10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(X_imgs[1])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(scaled_imgs[3])\n",
    "plt.title('Scale = 0.90')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(scaled_imgs[4])\n",
    "plt.title('Scale = 0.75')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(scaled_imgs[5])\n",
    "plt.title('Scale = 0.60')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "We would like our network to recognize the object present in any part of the image. Also, the object can be present partially in the corner or edges of the image. For this reason, we shift the object to various parts of the image. This may also result in addition of a background noise. The code snippet shows translating the image at four sides retaining 80 percent of the base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def get_translate_parameters(index):\n",
    "    if index == 0: # Translate left 20 percent\n",
    "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 1: # Translate right 20 percent\n",
    "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 2: # Translate top 20 percent\n",
    "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
    "    elif index == 3: # Translate bottom 20 percent\n",
    "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 4: # Translate left 10 percent\n",
    "        offset = np.array([0.0, 0.1], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.9 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.9 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 5: # Translate right 10 percent\n",
    "        offset = np.array([0.0, -0.1], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.9 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.9) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE \n",
    "    elif index == 6: # Translate top 10 percent\n",
    "        offset = np.array([0.1, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.9 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.9 * IMAGE_SIZE))  \n",
    "    elif index == 7: # Translate bottom 10 percent\n",
    "        offset = np.array([-0.1, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.9 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.9) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE      \n",
    "        \n",
    "    return offset, size, w_start, w_end, h_start, h_end\n",
    "\n",
    "def translate_images(X_imgs):\n",
    "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
    "    n_translations = 8\n",
    "    X_translated_arr = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_translations):\n",
    "            X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 3), \n",
    "\t\t\t\t    dtype = np.float32)\n",
    "            X_translated.fill(1.0) # Filling background color\n",
    "            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
    "            offsets[:, :] = base_offset \n",
    "            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
    "            \n",
    "            glimpses = sess.run(glimpses)\n",
    "            X_translated[:, h_start: h_start + size[0], \\\n",
    "\t\t\t w_start: w_start + size[1], :] = glimpses\n",
    "            X_translated_arr.extend(X_translated)\n",
    "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
    "    return X_translated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_imgs = translate_images(X_imgs)\n",
    "print(translated_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(1, 5)\n",
    "gs.update(wspace = 0.30, hspace = 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 16))\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(X_imgs[2])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(translated_imgs[2])\n",
    "plt.title('Left 20 percent')\n",
    "plt.subplot(gs[2])\n",
    "plt.imshow(translated_imgs[14])\n",
    "plt.title('Right 20 percent')\n",
    "plt.subplot(gs[3])\n",
    "plt.imshow(translated_imgs[26])\n",
    "plt.title('Top 20 percent')\n",
    "plt.subplot(gs[4])\n",
    "plt.imshow(translated_imgs[38])\n",
    "plt.title('Bottom 20 percent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs  ==  gridspec.GridSpec(1, 5)\n",
    "gs.update(wspace = 0.30, hspace = 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 16))\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(X_imgs[3])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(translated_imgs[3])\n",
    "plt.title('Left 20 percent')\n",
    "plt.subplot(gs[2])\n",
    "plt.imshow(translated_imgs[3])\n",
    "plt.title('Right 20 percent')\n",
    "plt.subplot(gs[3])\n",
    "plt.imshow(translated_imgs[3])\n",
    "plt.title('Top 20 percent')\n",
    "plt.subplot(gs[4])\n",
    "plt.imshow(translated_imgs[3])\n",
    "plt.title('Bottom 20 percent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation (at 90 degrees):\n",
    "The network has to recognize the object present in any orientation. Assuming the image is square, rotating the image at 90 degrees will not add any background noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    k = tf.placeholder(tf.int32)\n",
    "    tf_img = tf.image.rot90(X, k = k)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
    "                rotated_img = sess.run(tf_img, feed_dict = {X: img, k: i + 1})\n",
    "                X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return X_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_imgs = rotate_images(X_imgs)\n",
    "print(rotated_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(X_imgs[4])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(rotated_imgs[12])\n",
    "plt.title('Rotate 90 degrees')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(rotated_imgs[13])\n",
    "plt.title('Rotate 180 degrees')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(rotated_imgs[14])\n",
    "plt.title('Rotate 270 degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation (at finer angles):\n",
    "Depending upon the requirement, there maybe a necessity to orient the object at minute angles. However problem with this approach is, it will add background noise. If the background in image is of a fixed color (say white or black), the newly added background can blend with the image. However, if the newly added background color doesnâ€™t blend, the network may consider it as to be a feature and learn unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "def rotate_images(X_imgs, start_angle, end_angle, n_images):\n",
    "    X_rotate = []\n",
    "    iterate_at = (end_angle - start_angle) / (n_images - 1)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (None, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    radian = tf.placeholder(tf.float32, shape = (len(X_imgs)))\n",
    "    tf_img = tf.contrib.image.rotate(X, radian)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for index in range(n_images):\n",
    "            degrees_angle = start_angle + index * iterate_at\n",
    "            radian_value = degrees_angle * pi / 180  # Convert to radian\n",
    "            radian_arr = [radian_value] * len(X_imgs)\n",
    "            rotated_imgs = sess.run(tf_img, feed_dict = {X: X_imgs, radian: radian_arr})\n",
    "            X_rotate.extend(rotated_imgs)\n",
    "\n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return X_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start rotation at -90 degrees, end at 90 degrees and produce totally 14 images# Start  \n",
    "rotated_imgs = rotate_images(X_imgs, -90, 90, 14)\n",
    "print(rotated_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 16))\n",
    "gs = gridspec.GridSpec(3, 5)\n",
    "gs.update(wspace = 0.30, hspace = 0.0002)\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(X_imgs[5])\n",
    "plt.title('Base Image')\n",
    "\n",
    "for i in range(14):\n",
    "    plt.subplot(gs[i + 1])\n",
    "    plt.imshow(rotated_imgs[5 + 12 * i])\n",
    "    plt.title('Rotate {:.2f} degrees'.format(-90 + 13.846 * i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping:\n",
    "This scenario is more important for network to remove biasness of assuming certain features of the object is available in only a particular side. Consider the case shown in image example. You donâ€™t want network to learn that tilt of banana happens only in right side as observed in the base image. Also notice that flipping produces different set of images from rotation at multiple of 90 degrees.My additional question is has anyone done some study on what is the maximum number of classes it gives good performance. Consider, data can be generated with good amount of diversity for each class and time of training is not a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    tf_img2 = tf.image.flip_up_down(X)\n",
    "    tf_img3 = tf.image.transpose_image(X)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1, tf_img2, tf_img3], feed_dict = {X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    return X_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_images = flip_images(X_imgs)\n",
    "print(flipped_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(X_imgs[6])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(flipped_images[18])\n",
    "plt.title('Flip left right')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(flipped_images[19])\n",
    "plt.title('Flip up down')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(flipped_images[20])\n",
    "plt.title('Transpose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Salt and Pepper noise:\n",
    "Salt and Pepper noise refers to addition of white and black dots in the image. Though this may seem unnecessary, it is important to remember that a general user who is taking image to feed into your network may not be a professional photographer. His camera can produce blurry images with lots of white and black dots. This augmentation aides the above mentioned users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_pepper_noise(X_imgs):\n",
    "    # Need to produce a copy as to not modify the original image\n",
    "    X_imgs_copy = X_imgs.copy()\n",
    "    row, col, _ = X_imgs_copy[0].shape\n",
    "    salt_vs_pepper = 0.2\n",
    "    amount = 0.004\n",
    "    num_salt = np.ceil(amount * X_imgs_copy[0].size * salt_vs_pepper)\n",
    "    num_pepper = np.ceil(amount * X_imgs_copy[0].size * (1.0 - salt_vs_pepper))\n",
    "    \n",
    "    for X_img in X_imgs_copy:\n",
    "        # Add Salt noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1], :] = 1\n",
    "\n",
    "        # Add Pepper noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1], :] = 0\n",
    "    return X_imgs_copy\n",
    "  \n",
    "# salt_pepper_noise_imgs = add_salt_pepper_noise(X_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_pepper_noise_imgs = add_salt_pepper_noise(X_imgs)\n",
    "print(salt_pepper_noise_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(X_imgs[7])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(salt_pepper_noise_imgs[7])\n",
    "plt.title('Salt pepper noise image')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(X_imgs[8])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(salt_pepper_noise_imgs[8])\n",
    "plt.title('Salt pepper noise image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lighting condition:\n",
    "This is a very important type of diversity needed in the image dataset not only for the network to learn properly the object of interest but also to simulate the practical scenario of images being taken by the user. The lighting condition of the images are varied by adding Gaussian noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def add_gaussian_noise(X_imgs):\n",
    "    gaussian_noise_imgs = []\n",
    "    row, col, _ = X_imgs[0].shape\n",
    "    # Gaussian distribution parameters\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var ** 0.5\n",
    "    \n",
    "    for X_img in X_imgs:\n",
    "        gaussian = np.random.random((row, col, 1)).astype(np.float32)\n",
    "        gaussian = np.concatenate((gaussian, gaussian, gaussian), axis = 2)\n",
    "        gaussian_img = cv2.addWeighted(X_img, 0.75, 0.25 * gaussian, 0.25, 0)\n",
    "        gaussian_noise_imgs.append(gaussian_img)\n",
    "    gaussian_noise_imgs = np.array(gaussian_noise_imgs, dtype = np.float32)\n",
    "    return gaussian_noise_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_noise_imgs = add_gaussian_noise(X_imgs)\n",
    "print(gaussian_noise_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(X_imgs[9])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(gaussian_noise_imgs[9])\n",
    "plt.title('Shaded image')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(X_imgs[10])\n",
    "plt.title('Base Image')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(gaussian_noise_imgs[10])\n",
    "plt.title('Shaded image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective transform:\n",
    "In perspective transform, we try to project image from a different point of view. For this, the position of object should be known in advance. Merely calculating perspective transform without knowing the position of the object can lead to degradation of the dataset. Hence, this type of augmentation has to be performed selectively. The greatest advantage with this augmentation is that it can emphasize on parts of object in image which the network needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_coord(imshape):\n",
    "    vertices = np.array([[(0.09 * imshape[1], 0.99 * imshape[0]), \n",
    "                          (0.43 * imshape[1], 0.32 * imshape[0]), \n",
    "                          (0.56 * imshape[1], 0.32 * imshape[0]),\n",
    "                          (0.85 * imshape[1], 0.99 * imshape[0])]], dtype = np.int32)\n",
    "    return vertices\n",
    "\n",
    "def get_perspective_matrices(X_img):\n",
    "    offset = 15\n",
    "    img_size = (X_img.shape[1], X_img.shape[0])\n",
    "\n",
    "    # Estimate the coordinates of object of interest inside the image.\n",
    "    src = np.float32(get_mask_coord(X_img.shape))\n",
    "    dst = np.float32([[offset, img_size[1]], [offset, 0], [img_size[0] - offset, 0], \n",
    "                      [img_size[0] - offset, img_size[1]]])\n",
    "    \n",
    "    perspective_matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    return perspective_matrix\n",
    "\n",
    "def perspective_transform(X_img):\n",
    "    # Doing only for one type of example\n",
    "    perspective_matrix = get_perspective_matrices(X_img)\n",
    "    warped_img = cv2.warpPerspective(X_img, perspective_matrix,\n",
    "                                     (X_img.shape[1], X_img.shape[0]),\n",
    "                                     flags = cv2.INTER_LINEAR)\n",
    "    return warped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = X_imgs[11]\n",
    "perspective_img = perspective_transform(X_img)\n",
    "print(perspective_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_imgs[11])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(perspective_img)\n",
    "plt.title('Different View of Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save augmented images into class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gauge_0.png'\n",
    "path = './classes/psi_{0}/{1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(i, X_img):\n",
    "    file_resized = \"gauge_resized.jpg\"\n",
    "    filename_resized = path.format(i, file_resized)\n",
    "    scipy.misc.imsave(filename_resized, X_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_image(folder, img):\n",
    "    scale = [0.97,0.96,0.95,0.94,0.93,0.92,0.91,0.90,0.89,0.88,0.87,0.86,0.85,0.84,0.80,0.75,0.70,0.65,0.60]\n",
    "    scaled_imgs = central_scale_images(img, scale)\n",
    "\n",
    "    for i in range(0, 19):\n",
    "        filename = \"gauge_scale_{0}.jpg\".format(i)\n",
    "        filepath = path.format(folder, filename)\n",
    "        print(filepath)\n",
    "        scipy.misc.imsave(filepath, scaled_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlate_image(folder, img):\n",
    "    translated_imgs = translate_images(img)\n",
    "    \n",
    "    for i in range(0, 8):\n",
    "        filename = \"gauge_translate_{0}.jpg\".format(i)\n",
    "        filepath = path.format(folder, filename)\n",
    "        scipy.misc.imsave(filepath, translated_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_general_image(folder, img):\n",
    "    rotated = rotate_images(img, -90, 90, 14)\n",
    "    \n",
    "    for i in range(14):\n",
    "        filename = \"gauge_rotated_{0}.jpg\".format(i)\n",
    "        filepath = path.format(folder, filename)\n",
    "        scipy.misc.imsave(filepath, rotated[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipped_image(folder, img):\n",
    "    flipped = flip_images(img)\n",
    "    \n",
    "    for i in range(3):\n",
    "        filename = \"gauge_flipped_{0}.jpg\".format(i)\n",
    "        filepath = path.format(folder, filename)\n",
    "        scipy.misc.imsave(filepath, flipped[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salt_pepper(folder, img):\n",
    "    salt = add_salt_pepper_noise(img)\n",
    "    filename = \"gauge_salt_pepper.jpg\"\n",
    "    filepath = path.format(folder, filename)\n",
    "    scipy.misc.imsave(filepath, salt[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighting(folder, img):\n",
    "    gaussian = add_gaussian_noise(img)\n",
    "    filename = \"gauge_gaussian.jpg\"\n",
    "    filepath = path.format(folder, filename)\n",
    "    scipy.misc.imsave(filepath, gaussian[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = glob.glob(\"./classes/*/*.png\")\n",
    "X_img_paths = ['{}'.format(file) for file in dirList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_images(i, img):\n",
    "    # Resized Image\n",
    "    resize_image(i, img)\n",
    "    # Scale 97% - 60%\n",
    "    scaled_image(i, img)\n",
    "    # Tranlate the images\n",
    "    tranlate_image(i, img)\n",
    "    # Rotate the image 180, 270 degress\n",
    "    rotate_general_image(i, img)\n",
    "    # Flip the image\n",
    "    flipped_image(i, img)\n",
    "    # Add noise in pixels\n",
    "    salt_pepper(i, img)\n",
    "    # Lighting condition\n",
    "    lighting(i, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(0, 16)\n",
    "for idx, i in enumerate(r):\n",
    "    X_img = path.format(i, filename)\n",
    "    X_resize = tf_resize_images([X_img])\n",
    "    \n",
    "#     exec_images(i, X_resize)\n",
    "    \n",
    "    # Half Gauge Readings\n",
    "    if(i >= 1 and i < 15):\n",
    "        half = str(i + 0.5)\n",
    "        folder = half.replace(\".\",\"_\")\n",
    "        X_img = path.format(folder, filename)\n",
    "        X_resize = tf_resize_images([X_img])\n",
    "        \n",
    "        exec_images(folder, X_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
